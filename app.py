import streamlit as st
import sqlite3
import pandas as pd
from pykrx import stock
from datetime import datetime, timedelta
import re
import requests
import json
from urllib.parse import quote
import os
from dotenv import load_dotenv
import io
import time
import plotly.express as px
import plotly.graph_objects as go
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# --- í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---
load_dotenv()

# Streamlit Cloud secrets ìš°ì„ , ì—†ìœ¼ë©´ .env í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
NAVER_CLIENT_ID = st.secrets.get("NAVER_CLIENT_ID", os.getenv("NAVER_CLIENT_ID"))
NAVER_CLIENT_SECRET = st.secrets.get("NAVER_CLIENT_SECRET", os.getenv("NAVER_CLIENT_SECRET"))
GOOGLE_SHEET_PASSWORD = st.secrets.get("GOOGLE_SHEET_PASSWORD", os.getenv("GOOGLE_SHEET_PASSWORD", "default_password"))
SPREADSHEET_ID = st.secrets.get("GOOGLE_SPREADSHEET_ID", os.getenv("GOOGLE_SPREADSHEET_ID"))

st.set_page_config(
    page_title="ê¸‰ë“±ì£¼ íƒì§€ê¸° Pro",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSSë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì—¬ë°± ì¡°ì • ë° ìµœëŒ€ ë„ˆë¹„ ì„¤ì •
st.markdown("""
<style>
    .block-container {
        padding-top: 1rem;
        padding-bottom: 0rem;
        padding-left: 10rem;
        padding-right: 10rem;
        max-width: 100%;
    }
    .element-container {
        width: 100%;
        padding-left: 0;
        padding-right: 0;
    }
    .stDataFrame {
        width: 100%;
        padding-left: 0;
        padding-right: 0;
    }
    div[data-testid="stToolbar"] {
        display: none;
    }
    #MainMenu {
        visibility: hidden;
    }
    div[data-testid="stDecoration"] {
        display: none;
    }
    div[data-testid="stHeader"] {
        display: none;
    }
    h1 {
        font-size: 2.5rem !important;
        margin-bottom: 0.5rem !important;
        padding-left: 0.5rem;
    }
    h2 {
        font-size: 1.8rem !important;
        margin-top: 1rem !important;
        margin-bottom: 0.5rem !important;
        padding-left: 0.5rem;
    }
    h3 {
        font-size: 1.4rem !important;
        margin-top: 0.8rem !important;
        margin-bottom: 0.4rem !important;
        padding-left: 0.5rem;
    }
    h4 {
        font-size: 1.2rem !important;
        margin-top: 0.6rem !important;
        margin-bottom: 0.3rem !important;
        padding-left: 0.5rem;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 0;
        padding-left: 0.5rem;
    }
    .stTabs [data-baseweb="tab"] {
        padding: 0.5rem 1rem;
        margin: 0;
        font-size: 1.25rem !important;
        font-weight: bold !important;
        padding-top: 0.7rem !important;
        padding-bottom: 0.7rem !important;
    }
    .stTabs [data-baseweb="tab-panel"] {
        padding: 0.5rem 0;
    }
    .stDataFrame > div {
        padding-left: 0;
        padding-right: 0;
    }
    .stDataFrame > div > div {
        padding-left: 0;
        padding-right: 0;
    }
    .stDataFrame > div > div > div {
        padding-left: 0;
        padding-right: 0;
    }
    /* í…Œì´ë¸” ì…€ ë‚´ë¶€ ì—¬ë°± ì¡°ì • */
    .stDataFrame td, .stDataFrame th {
        padding: 0.3rem 0.5rem !important; /* ìƒí•˜ 0.3rem, ì¢Œìš° 0.5rem íŒ¨ë”© */
    }
</style>
""", unsafe_allow_html=True)

# ë¹ˆ ê³µê°„ ì¶”ê°€
st.markdown("<br>", unsafe_allow_html=True)

# ì•± ì œëª©
st.markdown("""
    <h1 style='text-align: center;'>ê¸‰ë“±ì£¼ íƒì§€ê¸° Pro</h1>
""", unsafe_allow_html=True)

def get_google_sheet():
    SCOPES = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    CREDENTIALS_FILE = 'credentials.json'
    SPREADSHEET_ID = st.secrets.get("GOOGLE_SPREADSHEET_ID", os.getenv("GOOGLE_SPREADSHEET_ID"))
    try:
        if not os.path.exists(CREDENTIALS_FILE):
            st.error(f"ì˜¤ë¥˜: {CREDENTIALS_FILE} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        if not SPREADSHEET_ID:
            st.error("ì˜¤ë¥˜: GOOGLE_SPREADSHEET_ID í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        credentials = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIALS_FILE, SCOPES)
        client = gspread.authorize(credentials)
        sheet = client.open_by_key(SPREADSHEET_ID)
        return sheet
    except Exception as e:
        st.error(f"êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def update_google_sheet(data_df, date_str):
    if data_df is None or data_df.empty:
        return False, "ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    try:
        sheet = get_google_sheet()
        if not sheet:
            return False, "êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨"
        year_month = f"{date_str[:4]}-{date_str[4:6]}"
        worksheet_name = f"{year_month}"
        try:
            worksheet = sheet.worksheet(worksheet_name)
        except gspread.exceptions.WorksheetNotFound:
            worksheet = sheet.add_worksheet(title=worksheet_name, rows=2000, cols=30)
        worksheet.clear()
        worksheet.update([list(data_df.columns)], f'A1')
        worksheet.update(data_df.values.tolist(), f'A2')
        return True, f"êµ¬ê¸€ ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì¶”ê°€ëœ ë°ì´í„°: {len(data_df)}ê°œ)"
    except Exception as e:
        return False, f"êµ¬ê¸€ ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"
    
def load_company_info_from_krx_url(krx_url, column_names_map):
    """KRXì—ì„œ ì œê³µí•˜ëŠ” URLë¡œë¶€í„° ìƒì¥ë²•ì¸ëª©ë¡ ë°ì´í„°ë¥¼ HTML í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(krx_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        df_company_info = None
        
        try:
            try:
                dfs = pd.read_html(io.StringIO(response.text), header=0)
            except UnicodeDecodeError:
                dfs = pd.read_html(io.StringIO(response.content.decode('cp949')), header=0)
            except Exception as e_decode_generic:
                dfs = pd.read_html(io.StringIO(response.content.decode('utf-8', errors='replace')), header=0)

            if not dfs:
                st.warning("HTMLì—ì„œ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return None
            df_company_info = dfs[0]
            
            ticker_col_name_in_html = column_names_map.get('source_ticker_col', 'ì¢…ëª©ì½”ë“œ')
            if ticker_col_name_in_html in df_company_info.columns:
                 df_company_info[ticker_col_name_in_html] = df_company_info[ticker_col_name_in_html].astype(str)
        except Exception as e_html:
            st.error(f"HTML í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹¤íŒ¨: {e_html}")
            return None
        
        if df_company_info is None or df_company_info.empty:
            st.warning("ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        required_source_cols = {
            'í‹°ì»¤': column_names_map.get('source_ticker_col', 'ì¢…ëª©ì½”ë“œ'),
            'ì—…ì¢…': column_names_map.get('source_industry_col', 'ì—…ì¢…'),
            'ì£¼ìš”ì œí’ˆ': column_names_map.get('source_products_col', 'ì£¼ìš”ì œí’ˆ')
        }

        cols_to_use = {} 
        for standard_col, source_col_name in required_source_cols.items():
            if source_col_name in df_company_info.columns:
                cols_to_use[source_col_name] = standard_col
            else:
                st.warning(f"ì›ë³¸ ë°ì´í„°ì— '{source_col_name}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. '{standard_col}' ì •ë³´ëŠ” ë¹„ì–´ìˆê²Œ ë©ë‹ˆë‹¤.")
        
        if 'í‹°ì»¤' not in cols_to_use.values():
             st.error(f"ì›ë³¸ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ì¸ '{required_source_cols['í‹°ì»¤']}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
             return None

        df_selected_info = df_company_info[list(cols_to_use.keys())].copy()
        df_selected_info.rename(columns=cols_to_use, inplace=True)
        
        for standard_col in ['í‹°ì»¤', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ']:
            if standard_col not in df_selected_info.columns:
                df_selected_info[standard_col] = ""
        
        if 'í‹°ì»¤' in df_selected_info.columns:
             df_selected_info['í‹°ì»¤'] = df_selected_info['í‹°ì»¤'].astype(str).str.strip().str.zfill(6)

        return df_selected_info[['í‹°ì»¤', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ']]

    except requests.exceptions.RequestException as e_req:
        st.error(f"KRX ê¸°ì—… ì •ë³´ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_req}")
        return None
    except Exception as e_general:
        st.error(f"KRX ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e_general}")
        return None

# --- ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ì˜ í—¬í¼ í•¨ìˆ˜ë“¤ ---
OUTPUT_COLUMNS_WITH_REMARKS = [
    'ë‚ ì§œ', 'í‹°ì»¤', 'ì¢…ëª©ëª…', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ', 'ì‹œì¥', 'ë¹„ê³ ',
    'ê¸°ì‚¬ì œëª©1', 'ê¸°ì‚¬ìš”ì•½1', 'ê¸°ì‚¬ë§í¬1',
    'ê¸°ì‚¬ì œëª©2', 'ê¸°ì‚¬ìš”ì•½2', 'ê¸°ì‚¬ë§í¬2',
    'ê¸°ì‚¬ì œëª©3', 'ê¸°ì‚¬ìš”ì•½3', 'ê¸°ì‚¬ë§í¬3',
    'ê¸°ì‚¬ì œëª©4', 'ê¸°ì‚¬ìš”ì•½4', 'ê¸°ì‚¬ë§í¬4',
    'ê¸°ì‚¬ì œëª©5', 'ê¸°ì‚¬ìš”ì•½5', 'ê¸°ì‚¬ë§í¬5'
]

def call_naver_search_api(query, display_count, client_id, client_secret):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not client_id or not client_secret:
        st.error("ì˜¤ë¥˜: Naver API Client ID ë˜ëŠ” Client Secretì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    encoded_query = quote(query)
    all_processed_items = []
    max_api_display_per_call = 100
    max_start_value = 1000
    num_calls_needed = min((display_count + max_api_display_per_call - 1) // max_api_display_per_call, 
                          (max_start_value + max_api_display_per_call - 1) // max_api_display_per_call)
    
    for i in range(num_calls_needed):
        try:
            start_index = 1 + (i * max_api_display_per_call)
            if start_index > max_start_value:
                break
                
            current_display_needed = min(max_api_display_per_call, display_count - len(all_processed_items))
            if current_display_needed <= 0:
                break
                
            api_url = f"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={current_display_needed}&start={start_index}&sort=date"
            
            headers = {
                "X-Naver-Client-Id": client_id,
                "X-Naver-Client-Secret": client_secret,
                "Content-Type": "application/json"
            }
            
            response = requests.get(api_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            if response.status_code == 200:
                news_data = response.json()
                
                if 'items' in news_data and news_data['items']:
                    for item in news_data['items']:
                        try:
                            # ë‚ ì§œ í˜•ì‹ì´ ë‹¤ì–‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ëŸ¬ í˜•ì‹ ì‹œë„
                            pub_date = None
                            date_formats = [
                                '%a, %d %b %Y %H:%M:%S %z',  # ê¸°ë³¸ í˜•ì‹
                                '%Y-%m-%d %H:%M:%S',         # ISO í˜•ì‹
                                '%Y%m%d'                     # ìˆ«ì í˜•ì‹
                            ]
                            
                            for date_format in date_formats:
                                try:
                                    pub_dt_object = datetime.strptime(item['pubDate'], date_format)
                                    pub_date = pub_dt_object.strftime('%Y%m%d')
                                    break
                                except ValueError:
                                    continue
                            
                            if pub_date:
                                item['pubDate'] = pub_date
                                # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
                                item['title'] = re.sub(r'<[^>]+>', '', item['title']).strip()
                                item['description'] = re.sub(r'<[^>]+>', '', item['description']).strip()
                                all_processed_items.append(item)
                            
                            if len(all_processed_items) >= display_count:
                                return all_processed_items
                                
                        except Exception as e:
                            st.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            continue
                    
                    if not news_data['items']:
                        break
                else:
                    if 'errorMessage' in news_data:
                        st.error(f"API ì˜¤ë¥˜ ë©”ì‹œì§€: {news_data['errorMessage']}")
                    break
                    
            # API í˜¸ì¶œ ì œí•œì— ê±¸ë ¸ì„ ë•Œ ëŒ€ê¸°
            time.sleep(0.05)  # 50msë¡œ ê°ì†Œ
                
        except requests.exceptions.RequestException as e:
            st.warning(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            time.sleep(0.5)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 500msë¡œ ê°ì†Œ
            continue
        except json.JSONDecodeError as e:
            st.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            continue
        except Exception as e:
            st.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            continue
            
    return all_processed_items

def extract_featured_stock_names_from_news(news_articles_list, target_date_str, all_stock_names_set):
    """ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ íŠ¹ì§•ì£¼ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. íŠ¹ì§•ì£¼ ê´€ë ¨ ì²« ë²ˆì§¸ ê¸°ì‚¬ë§Œ ì €ì¥í•©ë‹ˆë‹¤."""
    featured_stock_info = {}
    if not all_stock_names_set:
        st.warning("ì¢…ëª©ëª… ëª©ë¡ì´ ë¹„ì–´ìˆì–´ ë‰´ìŠ¤ì—ì„œ ì¢…ëª©ëª…ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return featured_stock_info
        
    filtered_articles_count = 0
    articles_on_target_date = 0
    
    for article in news_articles_list:
        article_pub_date = article.get("pubDate", "")
        if article_pub_date == target_date_str:
            articles_on_target_date += 1
            title = article.get("title", "")
            description = article.get("description", "")
            
            # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
            title_cleaned = re.sub(r'<[^>]+>', '', title).strip()
            description_cleaned = re.sub(r'<[^>]+>', '', description).strip()
            
            # íŠ¹ì§•ì£¼ í‚¤ì›Œë“œ í™•ì¥
            if any(keyword in title_cleaned for keyword in ["[íŠ¹ì§•ì£¼]", "íŠ¹ì§•ì£¼", "ê¸‰ë“±ì£¼", "ìƒí•œê°€", "ê°•ì„¸", "ìƒìŠ¹"]):
                filtered_articles_count += 1
                
                # ì¢…ëª©ëª…ì´ ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— ìˆëŠ”ì§€ í™•ì¸
                for stock_name in all_stock_names_set:
                    if stock_name in title_cleaned or stock_name in description_cleaned:
                        # í•´ë‹¹ ì¢…ëª©ì˜ ì²« ë²ˆì§¸ íŠ¹ì§•ì£¼ ê¸°ì‚¬ë§Œ ì €ì¥
                        if stock_name not in featured_stock_info:
                            featured_stock_info[stock_name] = [{
                                'title': title_cleaned,
                                'description': description_cleaned,
                                'link': article.get('link', '')
                            }]
    
    if articles_on_target_date == 0:
        st.warning(f"{target_date_str} ë‚ ì§œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.info(f"ì „ì²´ {articles_on_target_date}ê°œ ê¸°ì‚¬ ì¤‘ {filtered_articles_count}ê°œì˜ íŠ¹ì§•ì£¼ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    return featured_stock_info

# KRX ê¸°ì—… ì •ë³´ ë¡œë“œ
krx_company_list_url = "https://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13"
krx_column_names_map = {
    'source_ticker_col': 'ì¢…ëª©ì½”ë“œ',
    'source_industry_col': 'ì—…ì¢…',
    'source_products_col': 'ì£¼ìš”ì œí’ˆ'
}
company_details_df_global = load_company_info_from_krx_url(krx_company_list_url, krx_column_names_map)
if company_details_df_global is None:
    st.warning("ì•± ì‹œì‘ ì‹œ KRX ê¸°ì—… ì •ë³´ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ'ì€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    company_details_df_global = pd.DataFrame(columns=['í‹°ì»¤', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ'])

def get_all_market_data_with_names(date_str, company_info_df):
    """íŠ¹ì • ë‚ ì§œì˜ ì „ì²´ ì‹œì¥ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ì¢…ëª©ëª…ì„ í¬í•¨í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    all_data_frames = []
    company_info_cols_to_add = ['ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ']
    base_output_columns = ['í‹°ì»¤', 'ì¢…ëª©ëª…', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ', 'ì‹œì¥']
    markets_to_fetch = {"KOSPI": "KOSPI", "KOSDAQ": "KOSDAQ", "KONEX": "KONEX"}

    for market_code, market_name in markets_to_fetch.items():
        try:
            # ê¸°ë³¸ ì‹œì¥ ë°ì´í„° ì¡°íšŒ
            df_market_raw = stock.get_market_ohlcv(date_str, market=market_code)
            if not df_market_raw.empty and 'ë“±ë½ë¥ ' in df_market_raw.columns:
                df_market = df_market_raw.reset_index()
                if 'í‹°ì»¤' not in df_market.columns and 'index' in df_market.columns:
                     df_market.rename(columns={'index': 'í‹°ì»¤'}, inplace=True)

                df_market['ì‹œì¥'] = market_name
                df_market['í‹°ì»¤'] = df_market['í‹°ì»¤'].astype(str).str.zfill(6)
                
                # ì¢…ëª©ëª… ë§¤í•‘
                name_map = {ticker: stock.get_market_ticker_name(ticker) for ticker in df_market['í‹°ì»¤'] if stock.get_market_ticker_name(ticker)}
                df_market['ì¢…ëª©ëª…'] = df_market['í‹°ì»¤'].map(name_map)
                
                # íšŒì‚¬ ì •ë³´ ë³‘í•©
                if company_info_df is not None and not company_info_df.empty:
                    df_market = pd.merge(df_market, company_info_df, on="í‹°ì»¤", how="left")
                
                # ì—…ì¢…, ì£¼ìš”ì œí’ˆ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”
                for col in company_info_cols_to_add:
                    if col not in df_market.columns:
                        df_market[col] = ""
                
                current_output_cols = base_output_columns + company_info_cols_to_add
                all_data_frames.append(df_market[current_output_cols])
        except Exception as e:
            st.error(f"{market_name} ì „ì²´ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({date_str}): {e}")

    if not all_data_frames:
        st.warning(f"{date_str} ë‚ ì§œì— ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ì „ì²´ ì‹œì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    combined_df = pd.concat(all_data_frames, ignore_index=True) 
    for col in ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']:
        if col in combined_df.columns:
            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')
    for col in company_info_cols_to_add:
        if col in combined_df.columns:
            combined_df[col] = combined_df[col].fillna("")
        else:
            combined_df[col] = ""
    combined_df = combined_df.dropna(subset=['ë“±ë½ë¥ ', 'ì¢…ëª©ëª…'])
    if combined_df.empty:
        st.warning(f"{date_str} ë‚ ì§œì— ìœ íš¨í•œ ì „ì²´ ì‹œì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    return combined_df

def is_valid_date_format(date_string):
    if not re.match(r"^\d{8}$", date_string): return False
    try:
        datetime.strptime(date_string, '%Y%m%d')
        return True
    except ValueError: return False

def initialize_article_columns(stock_info, start_idx=1, end_idx=5):
    """ê¸°ì‚¬ ì»¬ëŸ¼ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    for i in range(start_idx, end_idx + 1):
        stock_info[f'ê¸°ì‚¬ì œëª©{i}'] = ''
        stock_info[f'ê¸°ì‚¬ìš”ì•½{i}'] = ''
        stock_info[f'ê¸°ì‚¬ë§í¬{i}'] = ''
    return stock_info

def search_stock_articles_by_date(stock_name, client_id, client_secret, target_date_str, max_count=5, max_retries=3, delay=0.3, match_date=False):
    """ì¢…ëª©ëª…ìœ¼ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰í•˜ì—¬ ê¸°ì‚¬ ìµœëŒ€ max_countê°œ ë°˜í™˜"""
    for attempt in range(max_retries):
        try:
            time.sleep(delay)
            encoded_query = quote(stock_name)
            api_url = f"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display=100&start=1&sort=date"
            headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code == 429:
                time.sleep(delay * 1.5)  # 429 ì˜¤ë¥˜ ì‹œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€ìœ¨ ê°ì†Œ
                delay *= 1.5
                continue
            response.raise_for_status()
            news_data = response.json()
            result = []
            if 'items' in news_data and news_data['items']:
                if match_date:
                    for item in news_data['items']:
                        try:
                            pub_dt_object = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S %z')
                            pub_date_str = pub_dt_object.strftime('%Y%m%d')
                            if pub_date_str == target_date_str:
                                result.append({
                                    'title': re.sub(r'<[^>]+>', '', item['title']).strip(),
                                    'description': re.sub(r'<[^>]+>', '', item['description']).strip(),
                                    'link': item['link']
                                })
                                if len(result) >= max_count:
                                    break
                        except Exception:
                            continue
                else:
                    for item in news_data['items'][:max_count]:
                        result.append({
                            'title': re.sub(r'<[^>]+>', '', item['title']).strip(),
                            'description': re.sub(r'<[^>]+>', '', item['description']).strip(),
                            'link': item['link']
                        })
            return result  # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        except requests.exceptions.RequestException:
            if attempt < max_retries - 1:
                time.sleep(delay * (attempt + 0.5))  # ì¬ì‹œë„ ì‹œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€ìœ¨ ê°ì†Œ
            continue
        except Exception:
            return []  # ì—ëŸ¬ ë°œìƒì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    return []  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

@st.cache_data
def get_excel_data(df, date_str):
    excel_file = io.BytesIO()
    with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='ë¶„ì„ê²°ê³¼')
    excel_file.seek(0)
    return excel_file.getvalue()

@st.cache_data
def get_txt_data(df):
    return df.to_csv(sep='\t', index=False)

def format_number(x):
    if pd.isna(x):
        return ""
    try:
        return f"{x:,.0f}"
    except:
        return str(x)

def format_percentage(x):
    if pd.isna(x):
        return ""
    try:
        return f"{x:,.2f}%"
    except:
        return str(x)

def color_negative_red(val):
    """ìˆ«ì ê°’ì— ë”°ë¼ ìƒ‰ìƒì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if pd.isna(val):
            return None
        value = float(str(val).replace('%', ''))
        return 'color: red' if value < 0 else 'color: green'
    except:
        return None

def display_analysis_results(final_df_sorted, date_str, all_market_data_df, top_n_count):
    # ê²°ê³¼ í‘œì‹œ
    st.success(f"ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(final_df_sorted):,}ê°œ ì¢…ëª©)")

    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYYMMDD -> YYYYë…„ MMì›” DDì¼)
    formatted_date = f"{date_str[:4]}ë…„ {date_str[4:6]}ì›” {date_str[6:8]}ì¼"
    st.markdown(f"""
        <h2 style='text-align: center;'>{formatted_date} ì‹œì¥ ë¶„ì„</h3>
    """, unsafe_allow_html=True)

    # ì „ì²´ ì¢…ëª©ê³¼ ê¸‰ë“±ì£¼+íŠ¹ì§•ì£¼ ë¶„ì„ì„ íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2 = st.tabs(["ê¸‰ë“±ì£¼+íŠ¹ì§•ì£¼ ë¶„ì„", "ì „ì²´ ì¢…ëª© ë¶„ì„"])
    
    with tab1:
        st.subheader("ê¸‰ë“±ì£¼+íŠ¹ì§•ì£¼ ë¶„ì„")
        
        # ìƒë‹¨ ì§€í‘œ ì¹´ë“œë“¤ì„ í•œ ì¤„ì— ë°°ì¹˜
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            high_volume_count = len(final_df_sorted[final_df_sorted['ê±°ë˜ëŒ€ê¸ˆ'] >= 10000000000])
            st.metric("ê±°ë˜ëŒ€ê¸ˆ 100ì–µ ì´ìƒ", f"{high_volume_count:,}")
        
        with col2:
            top_featured_count = len(final_df_sorted[final_df_sorted['ë¹„ê³ '] == f"top{top_n_count}+íŠ¹ì§•ì£¼"])
            st.metric("Top N + íŠ¹ì§•ì£¼", f"{top_featured_count:,}")
        
        with col3:
            featured_count = len(final_df_sorted[final_df_sorted['ë¹„ê³ '] == "íŠ¹ì§•ì£¼"])
            st.metric("íŠ¹ì§•ì£¼", f"{featured_count:,}")
        
        with col4:
            total_count = len(final_df_sorted)
            st.metric("ì „ì²´ ë¶„ì„ ì¢…ëª©", f"{total_count:,}")
        
        # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
        display_df = final_df_sorted.copy()
        numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']
        for col in numeric_columns:
            if col in display_df.columns:
                display_df[col] = display_df[col].apply(format_number)
        display_df['ë“±ë½ë¥ '] = display_df['ë“±ë½ë¥ '].apply(format_percentage)
        
        # ê±°ë˜ëŒ€ê¸ˆ 100ì–µ ì´ìƒ ê°•ì¡°ë¥¼ ìœ„í•œ ìŠ¤íƒ€ì¼ í•¨ìˆ˜
        def highlight_high_amount(val):
            try:
                amount = float(str(val).replace(',', ''))
                return 'color: #FF0000' if amount >= 10000000000 else None
            except:
                return None
        
        styled_df = display_df.style.map(color_negative_red, subset=['ë“±ë½ë¥ ']).map(highlight_high_amount, subset=['ê±°ë˜ëŒ€ê¸ˆ'])
        st.dataframe(styled_df, use_container_width=True)

        # í•˜ë‹¨ì—ë§Œ ë‹¤ìš´ë¡œë“œ/ì €ì¥ ë²„íŠ¼
        st.subheader("ê¸‰ë“±ì£¼+íŠ¹ì§•ì£¼ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
        excel_data = get_excel_data(final_df_sorted, date_str)
        txt_data = get_txt_data(final_df_sorted)
        col1, col2, col3, col4 = st.columns([1,1,1,1])
        with col1:
            st.download_button(
                label="Excel ë‹¤ìš´ë¡œë“œ",
                data=excel_data,
                file_name=f"stock_analysis_{date_str}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key=f"excel_download_{date_str}"
            )
        with col2:
            st.download_button(
                label="TXT ë‹¤ìš´ë¡œë“œ",
                data=txt_data,
                file_name=f"stock_analysis_{date_str}.txt",
                mime="text/plain",
                key=f"txt_download_{date_str}"
            )
        with col3:
            if st.button("êµ¬ê¸€ ì‹œíŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°", key=f"google_sheet_{date_str}"):
                with st.spinner("êµ¬ê¸€ ì‹œíŠ¸ë¡œ ë‚´ë³´ë‚´ëŠ” ì¤‘..."):
                    success, msg = update_google_sheet(final_df_sorted, date_str)
                    if success:
                        st.success(msg)
                    else:
                        st.error(msg)
        with col4:
            if 'db_save_state' not in st.session_state:
                st.session_state.db_save_state = None
            if 'db_overwrite_state' not in st.session_state:
                st.session_state.db_overwrite_state = None
            if st.button("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥", key=f"db_save_{date_str}"):
                st.session_state.db_save_state = "checking"
                st.session_state.selected_tab = "ë°ì´í„°ë² ì´ìŠ¤"
                st.rerun()
            if st.session_state.db_save_state == "checking":
                success, message = save_to_database(final_df_sorted)
                if message == "already_exists":
                    st.warning("ì´ë¯¸ ì €ì¥ëœ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ?")
                    overwrite_col1, overwrite_col2 = st.columns(2)
                    with overwrite_col1:
                        if st.button("ë®ì–´ì“°ê¸°", key=f"overwrite_{date_str}"):
                            st.session_state.db_overwrite_state = True
                            st.rerun()
                    with overwrite_col2:
                        if st.button("ì·¨ì†Œ", key=f"cancel_{date_str}"):
                            st.session_state.db_save_state = None
                            st.session_state.db_overwrite_state = None
                            st.rerun()
                elif success:
                    st.success(message)
                    st.session_state.db_save_state = None
                else:
                    st.error(message)
                    st.session_state.db_save_state = None
            if st.session_state.db_overwrite_state:
                with st.spinner("ë°ì´í„°ë¥¼ ë®ì–´ì“°ëŠ” ì¤‘..."):
                    success, message = save_to_database(final_df_sorted, overwrite=True)
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
                    st.session_state.db_save_state = None
                    st.session_state.db_overwrite_state = None
                    st.rerun()

    with tab2:
        st.subheader(f"ì „ì²´ ì¢…ëª© (ì´ {len(all_market_data_df):,}ê°œ ì¢…ëª©)")

        # ì „ì²´ ì‹œì¥ ë°ì´í„° í‘œì‹œ (ê°€ì¥ ìœ„ë¡œ)
        market_display_df = all_market_data_df.copy()
        numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']
        for col in numeric_columns:
            if col in market_display_df.columns:
                market_display_df[col] = market_display_df[col].apply(format_number)
        market_display_df['ë“±ë½ë¥ '] = market_display_df['ë“±ë½ë¥ '].apply(format_percentage)
        styled_market_df = market_display_df.style.map(color_negative_red, subset=['ë“±ë½ë¥ '])
        st.dataframe(styled_market_df, use_container_width=True, height=400)
        st.markdown('<div style="height: 24px;"></div>', unsafe_allow_html=True)

        # ë“±ë½ë¥  Top30, ê±°ë˜ëŒ€ê¸ˆ Top30 ë°ì´í„°
        top30_rate = all_market_data_df.nlargest(30, 'ë“±ë½ë¥ ')
        top30_amount = all_market_data_df.nlargest(30, 'ê±°ë˜ëŒ€ê¸ˆ')

        # 4ê°œ ì»¬ëŸ¼ìœ¼ë¡œ í•œ ì¤„ì— ë°°ì¹˜
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.markdown("<div style='text-align:center; font-weight:bold; font-size:1.1em;'>ë“±ë½ë¥  Top30</div>", unsafe_allow_html=True)
            top30_rate_table = top30_rate[['ì¢…ëª©ëª…', 'ë“±ë½ë¥ ', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ']].copy()
            top30_rate_table['ë“±ë½ë¥ '] = top30_rate_table['ë“±ë½ë¥ '].apply(format_percentage)
            st.dataframe(
                top30_rate_table.style.set_table_styles([
                    {'selector': 'td', 'props': [('font-size', '0.95em')]},
                    {'selector': 'th', 'props': [('font-size', '0.95em')]}
                ]),
                use_container_width=True, hide_index=True
            )

        with col2:
            st.markdown("<div style='text-align:center; font-weight:bold; font-size:1.1em;'>ë“±ë½ë¥  Top30 ì‹œì¥ë³„ ë¶„í¬</div>", unsafe_allow_html=True)
            fig1 = px.pie(top30_rate, names='ì‹œì¥', title=None)
            st.plotly_chart(fig1, use_container_width=True)

        with col3:
            st.markdown("<div style='text-align:center; font-weight:bold; font-size:1.1em;'>ê±°ë˜ëŒ€ê¸ˆ Top30</div>", unsafe_allow_html=True)
            top30_amount_table = top30_amount[['ì¢…ëª©ëª…', 'ê±°ë˜ëŒ€ê¸ˆ', 'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ']].copy()
            top30_amount_table['ê±°ë˜ëŒ€ê¸ˆ'] = top30_amount_table['ê±°ë˜ëŒ€ê¸ˆ'].apply(format_number)
            st.dataframe(
                top30_amount_table.style.set_table_styles([
                    {'selector': 'td', 'props': [('font-size', '0.95em')]},
                    {'selector': 'th', 'props': [('font-size', '0.95em')]}
                ]),
                use_container_width=True, hide_index=True
            )

        with col4:
            st.markdown("<div style='text-align:center; font-weight:bold; font-size:1.1em;'>ê±°ë˜ëŒ€ê¸ˆ Top30 ì‹œì¥ë³„ ë¶„í¬</div>", unsafe_allow_html=True)
            fig2 = px.pie(top30_amount, names='ì‹œì¥', title=None)
            st.plotly_chart(fig2, use_container_width=True)

        # ì „ì²´ ì‹œì¥ ê±°ë˜ëŒ€ê¸ˆ í‘œì‹œ
        try:
            df_total_investor = stock.get_market_trading_value_by_date(date_str, date_str, "ALL", etf=True, etn=True, elw=True)
            if not df_total_investor.empty:
                # 'ì „ì²´' ì»¬ëŸ¼ ì œê±°
                if 'ì „ì²´' in df_total_investor.columns:
                    df_total_investor = df_total_investor.drop(columns=['ì „ì²´'])
                # 'ë‚ ì§œ' ì»¬ëŸ¼ ì œê±°
                if 'ë‚ ì§œ' in df_total_investor.columns:
                    df_total_investor = df_total_investor.drop(columns=['ë‚ ì§œ'])
                # ê°€ê³µëœ í…Œì´ë¸”ë§Œ ì¶œë ¥
                styled_df = df_total_investor.copy()
                for col in styled_df.columns:
                    if pd.api.types.is_numeric_dtype(styled_df[col]):
                        styled_df[col] = styled_df[col].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "")
                def style_color(val):
                    try:
                        v = float(str(val).replace(',', ''))
                        if v > 0:
                            return 'color: #FF0000'
                        elif v < 0:
                            return 'color: #0000FF'
                    except:
                        return None
                st.markdown("#### íˆ¬ììë³„ ê±°ë˜ëŒ€ê¸ˆ(KOSPI+KOSDAQ+KONEX)")
                st.dataframe(styled_df.style.map(style_color), use_container_width=True)
        except Exception as e:
            st.warning(f"ì „ì²´ ì‹œì¥ ê±°ë˜ëŒ€ê¸ˆ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # ì‹œì¥ë³„ íˆ¬ìì ì •ë³´ í‘œì‹œ (KOSPI/KOSDAQ í†µí•©)
        try:
            df_kospi = stock.get_market_trading_value_by_date(date_str, date_str, "KOSPI", etf=True, etn=True, elw=True, detail=True)
            df_kosdaq = stock.get_market_trading_value_by_date(date_str, date_str, "KOSDAQ", etf=True, etn=True, elw=True, detail=True)
            if not df_kospi.empty:
                df_kospi["ì‹œì¥"] = "KOSPI"
            if not df_kosdaq.empty:
                df_kosdaq["ì‹œì¥"] = "KOSDAQ"
            # ê³µí†µ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            if not df_kospi.empty and not df_kosdaq.empty:
                common_cols = list(set(df_kospi.columns) & set(df_kosdaq.columns))
                # ì‹œì¥ ì»¬ëŸ¼ì´ ë§¨ ì•ìœ¼ë¡œ ì˜¤ë„ë¡
                if "ì‹œì¥" in common_cols:
                    common_cols = ["ì‹œì¥"] + [col for col in df_kospi.columns if col != "ì‹œì¥" and col in common_cols]
                df_merged = pd.concat([df_kospi[common_cols], df_kosdaq[common_cols]], ignore_index=True)
            elif not df_kospi.empty:
                df_merged = df_kospi.copy()
            elif not df_kosdaq.empty:
                df_merged = df_kosdaq.copy()
            else:
                df_merged = pd.DataFrame()
            if not df_merged.empty:
                # 'ì „ì²´' ì»¬ëŸ¼ ì œê±°
                if 'ì „ì²´' in df_merged.columns:
                    df_merged = df_merged.drop(columns=['ì „ì²´'])
                # ìˆ«ì í¬ë§· ë° ìƒ‰ìƒ ìŠ¤íƒ€ì¼
                for col in df_merged.columns:
                    if pd.api.types.is_numeric_dtype(df_merged[col]):
                        df_merged[col] = df_merged[col].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "")
                def style_color(val):
                    try:
                        v = float(str(val).replace(',', ''))
                        if v > 0:
                            return 'color: #FF0000'
                        elif v < 0:
                            return 'color: #0000FF'
                    except:
                        return None
                st.markdown("#### KOSPI/KOSDAQ íˆ¬ììë³„ ê±°ë˜ëŒ€ê¸ˆ")
                st.dataframe(df_merged.style.map(style_color), use_container_width=True, hide_index=True)
        except Exception as e:
            st.warning(f"KOSPI/KOSDAQ íˆ¬ìì ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def init_database():
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    try:
        conn = sqlite3.connect('stock_analysis.db')
        c = conn.cursor()
        
        # í…Œì´ë¸”ì´ ì—†ì„ ë•Œë§Œ ìƒì„± (í…Œë§ˆ, AI_í•œì¤„ìš”ì•½ ì»¬ëŸ¼ í¬í•¨)
        c.execute('''
            CREATE TABLE IF NOT EXISTS stock_analysis (
                ë‚ ì§œ TEXT,
                í‹°ì»¤ TEXT,
                ì¢…ëª©ëª… TEXT,
                ì—…ì¢… TEXT,
                ì£¼ìš”ì œí’ˆ TEXT,
                ì‹œê°€ REAL,
                ê³ ê°€ REAL,
                ì €ê°€ REAL,
                ì¢…ê°€ REAL,
                ë“±ë½ë¥  REAL,
                ê±°ë˜ëŸ‰ INTEGER,
                ê±°ë˜ëŒ€ê¸ˆ INTEGER,
                ì‹œì¥ TEXT,
                ë¹„ê³  TEXT,
                ê¸°ì‚¬ì œëª©1 TEXT,
                ê¸°ì‚¬ìš”ì•½1 TEXT,
                ê¸°ì‚¬ë§í¬1 TEXT,
                ê¸°ì‚¬ì œëª©2 TEXT,
                ê¸°ì‚¬ìš”ì•½2 TEXT,
                ê¸°ì‚¬ë§í¬2 TEXT,
                ê¸°ì‚¬ì œëª©3 TEXT,
                ê¸°ì‚¬ìš”ì•½3 TEXT,
                ê¸°ì‚¬ë§í¬3 TEXT,
                ê¸°ì‚¬ì œëª©4 TEXT,
                ê¸°ì‚¬ìš”ì•½4 TEXT,
                ê¸°ì‚¬ë§í¬4 TEXT,
                ê¸°ì‚¬ì œëª©5 TEXT,
                ê¸°ì‚¬ìš”ì•½5 TEXT,
                ê¸°ì‚¬ë§í¬5 TEXT,
                PRIMARY KEY (ë‚ ì§œ, í‹°ì»¤)
            )
        ''')
        
        # ê¸°ì¡´ í…Œì´ë¸”ì— ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ (ì´ë¯¸ ìˆìœ¼ë©´ ë¬´ì‹œ)
        for col in ["í…Œë§ˆ", "AI_í•œì¤„ìš”ì•½"]:
            try:
                c.execute(f"ALTER TABLE stock_analysis ADD COLUMN {col} TEXT")
            except sqlite3.OperationalError:
                pass
        
        # ê¸°ì¡´ í…Œì´ë¸”ì— ìƒˆ ê¸°ì‚¬ ì»¬ëŸ¼ ì¶”ê°€
        try:
            c.execute("ALTER TABLE stock_analysis ADD COLUMN ê¸°ì‚¬ì œëª©4 TEXT")
            c.execute("ALTER TABLE stock_analysis ADD COLUMN ê¸°ì‚¬ìš”ì•½4 TEXT")
            c.execute("ALTER TABLE stock_analysis ADD COLUMN ê¸°ì‚¬ë§í¬4 TEXT")
            c.execute("ALTER TABLE stock_analysis ADD COLUMN ê¸°ì‚¬ì œëª©5 TEXT")
            c.execute("ALTER TABLE stock_analysis ADD COLUMN ê¸°ì‚¬ìš”ì•½5 TEXT")
            c.execute("ALTER TABLE stock_analysis ADD COLUMN ê¸°ì‚¬ë§í¬5 TEXT")
        except sqlite3.OperationalError as e:
            # ì»¬ëŸ¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë¬´ì‹œ
            pass
        
        conn.commit()
        conn.close()
    except Exception as e:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if conn:
            conn.close()

def reset_database():
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì™„ì „íˆ ì´ˆê¸°í™” (ëª¨ë“  ë°ì´í„° ì‚­ì œ)"""
    try:
        conn = sqlite3.connect('stock_analysis.db')
        c = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
        c.execute("DROP TABLE IF EXISTS stock_analysis")
        
        # í…Œì´ë¸” ë‹¤ì‹œ ìƒì„±
        c.execute('''
            CREATE TABLE IF NOT EXISTS stock_analysis (
                ë‚ ì§œ TEXT,
                í‹°ì»¤ TEXT,
                ì¢…ëª©ëª… TEXT,
                ì—…ì¢… TEXT,
                ì£¼ìš”ì œí’ˆ TEXT,
                ì‹œê°€ REAL,
                ê³ ê°€ REAL,
                ì €ê°€ REAL,
                ì¢…ê°€ REAL,
                ë“±ë½ë¥  REAL,
                ê±°ë˜ëŸ‰ INTEGER,
                ê±°ë˜ëŒ€ê¸ˆ INTEGER,
                ì‹œì¥ TEXT,
                ë¹„ê³  TEXT,
                ê¸°ì‚¬ì œëª©1 TEXT,
                ê¸°ì‚¬ìš”ì•½1 TEXT,
                ê¸°ì‚¬ë§í¬1 TEXT,
                ê¸°ì‚¬ì œëª©2 TEXT,
                ê¸°ì‚¬ìš”ì•½2 TEXT,
                ê¸°ì‚¬ë§í¬2 TEXT,
                ê¸°ì‚¬ì œëª©3 TEXT,
                ê¸°ì‚¬ìš”ì•½3 TEXT,
                ê¸°ì‚¬ë§í¬3 TEXT,
                ê¸°ì‚¬ì œëª©4 TEXT,
                ê¸°ì‚¬ìš”ì•½4 TEXT,
                ê¸°ì‚¬ë§í¬4 TEXT,
                ê¸°ì‚¬ì œëª©5 TEXT,
                ê¸°ì‚¬ìš”ì•½5 TEXT,
                ê¸°ì‚¬ë§í¬5 TEXT,
                PRIMARY KEY (ë‚ ì§œ, í‹°ì»¤)
            )
        ''')
        
        conn.commit()
        conn.close()
        st.success("ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if conn:
            conn.close()

def save_to_database(df, overwrite=False):
    """ë°ì´í„°í”„ë ˆì„ì„ SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    if df is None or df.empty:
        return False, "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    conn = None
    try:
        conn = sqlite3.connect('stock_analysis.db')
        cursor = conn.cursor()
        date_str = df['ë‚ ì§œ'].iloc[0]

        # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute("SELECT COUNT(*) FROM stock_analysis WHERE ë‚ ì§œ = ?", (date_str,))
        exists = cursor.fetchone()[0] > 0

        if exists and not overwrite:
            return False, "already_exists"

        # ë®ì–´ì“°ê¸° ëª¨ë“œì´ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
        if exists and overwrite:
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            cursor.execute("DELETE FROM stock_analysis WHERE ë‚ ì§œ = ?", (date_str,))
            conn.commit()
            st.info(f"{date_str} ë‚ ì§œì˜ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

        # ë°ì´í„° ì „ì²˜ë¦¬
        df_to_save = df.copy()
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ ë³€í™˜
        numeric_columns = {
            'float': ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ë“±ë½ë¥ '],
            'int': ['ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']
        }
        
        for col in numeric_columns['float']:
            df_to_save[col] = pd.to_numeric(df_to_save[col], errors='coerce')
        
        for col in numeric_columns['int']:
            df_to_save[col] = pd.to_numeric(df_to_save[col], errors='coerce').fillna(0).astype('int64')
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì „ì²˜ë¦¬
        string_columns = [col for col in df_to_save.columns if col not in 
                         numeric_columns['float'] + numeric_columns['int']]
        
        for col in string_columns:
            df_to_save[col] = df_to_save[col].fillna('').astype(str)

        # ë°ì´í„° ì €ì¥
        placeholders = ','.join(['?' for _ in df_to_save.columns])
        insert_sql = f'''
            INSERT INTO stock_analysis (
                {','.join(df_to_save.columns)}
            ) VALUES ({placeholders})
        '''
        
        # ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í•œ ë²ˆì— ì €ì¥
        data_to_insert = df_to_save.values.tolist()
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì €ì¥
        chunk_size = 500
        for i in range(0, len(data_to_insert), chunk_size):
            chunk = data_to_insert[i:i + chunk_size]
            cursor.executemany(insert_sql, chunk)
            conn.commit()
        
        # ì €ì¥ëœ ë°ì´í„° ìˆ˜ í™•ì¸
        cursor.execute("SELECT COUNT(*) FROM stock_analysis WHERE ë‚ ì§œ = ?", (date_str,))
        saved_count = cursor.fetchone()[0]
        
        # ì›ë³¸ ë°ì´í„°ì™€ ì €ì¥ëœ ë°ì´í„° ìˆ˜ ë¹„êµ
        if saved_count != len(df):
            return False, f"ë°ì´í„° ì €ì¥ ë¶ˆì¼ì¹˜: ì›ë³¸ {len(df)}ê°œ, ì €ì¥ë¨ {saved_count}ê°œ"
        
        conn.close()
        return True, f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (ì €ì¥ëœ ë°ì´í„°: {saved_count}ê°œ)"
        
    except Exception as e:
        if conn:
            conn.close()
        return False, f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def get_saved_dates():
    """ì €ì¥ëœ ë‚ ì§œ ëª©ë¡ ì¡°íšŒ"""
    conn = sqlite3.connect('stock_analysis.db')
    c = conn.cursor()
    c.execute("SELECT DISTINCT ë‚ ì§œ FROM stock_analysis ORDER BY ë‚ ì§œ DESC")
    dates = [row[0] for row in c.fetchall()]
    conn.close()
    return dates

def get_analysis_by_date(date_str):
    """íŠ¹ì • ë‚ ì§œì˜ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        conn = sqlite3.connect('stock_analysis.db')
        query = "SELECT * FROM stock_analysis WHERE ë‚ ì§œ = ?"
        df = pd.read_sql_query(query, conn, params=(date_str,))
        conn.close()
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if conn:
            conn.close()
        return pd.DataFrame()

def get_data_by_date_range(start_date, end_date):
    """íŠ¹ì • ê¸°ê°„ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°íšŒ"""
    try:
        conn = sqlite3.connect('stock_analysis.db')
        query = "SELECT * FROM stock_analysis WHERE ë‚ ì§œ BETWEEN ? AND ?"
        df = pd.read_sql_query(query, conn, params=(start_date, end_date))
        conn.close()
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if conn:
            conn.close()
        return pd.DataFrame()

def create_market_distribution_pie(df):
    """ì‹œì¥ë³„ ì¢…ëª© ë¶„í¬ íŒŒì´ ì°¨íŠ¸ ìƒì„±"""
    market_counts = df.groupby('ì‹œì¥').size().reset_index(name='count')
    fig = px.pie(
        market_counts, 
        values='count', 
        names='ì‹œì¥',
        title='ì‹œì¥ë³„ ì¢…ëª© ë¶„í¬',
        color_discrete_sequence=px.colors.qualitative.Set3
    )
    fig.update_traces(textposition='inside', textinfo='percent+label')
    return fig

def create_top_rate_changes_bar(df):
    """ë“±ë½ë¥  ìƒìœ„ 10ê°œ ì¢…ëª© ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±"""
    # ê° ì¢…ëª©ì˜ ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
    latest_data = df.sort_values('ë‚ ì§œ').groupby('ì¢…ëª©ëª…').last()
    top_changes = latest_data.nlargest(10, 'ë“±ë½ë¥ ')[['ë“±ë½ë¥ ', 'ì‹œì¥']]
    
    fig = go.Figure()
    for market in top_changes['ì‹œì¥'].unique():
        market_data = top_changes[top_changes['ì‹œì¥'] == market]
        fig.add_trace(go.Bar(
            x=market_data.index,
            y=market_data['ë“±ë½ë¥ '],
            name=market,
            text=market_data['ë“±ë½ë¥ '].round(2).astype(str) + '%',
            textposition='auto',
        ))
    
    fig.update_layout(
        title='ë“±ë½ë¥  ìƒìœ„ 10ê°œ ì¢…ëª©',
        xaxis_title='ì¢…ëª©ëª…',
        yaxis_title='ë“±ë½ë¥  (%)',
        barmode='group',
        showlegend=True
    )
    return fig

def create_top_volume_bar(df):
    """ê±°ë˜ëŸ‰ ìƒìœ„ 10ê°œ ì¢…ëª© ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±"""
    # ê° ì¢…ëª©ì˜ ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
    latest_data = df.sort_values('ë‚ ì§œ').groupby('ì¢…ëª©ëª…').last()
    top_volume = latest_data.nlargest(10, 'ê±°ë˜ëŸ‰')[['ê±°ë˜ëŸ‰', 'ì‹œì¥']]
    
    fig = go.Figure()
    for market in top_volume['ì‹œì¥'].unique():
        market_data = top_volume[top_volume['ì‹œì¥'] == market]
        fig.add_trace(go.Bar(
            x=market_data.index,
            y=market_data['ê±°ë˜ëŸ‰'],
            name=market,
            text=(market_data['ê±°ë˜ëŸ‰'] / 1000000).round(2).astype(str) + 'M',
            textposition='auto',
        ))
    
    fig.update_layout(
        title='ê±°ë˜ëŸ‰ ìƒìœ„ 10ê°œ ì¢…ëª©',
        xaxis_title='ì¢…ëª©ëª…',
        yaxis_title='ê±°ë˜ëŸ‰',
        barmode='group',
        showlegend=True
    )
    return fig

def create_industry_distribution_bar(df):
    """ì—…ì¢…ë³„ ì¢…ëª© ìˆ˜ ë¶„í¬ ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±"""
    # ê° ì¢…ëª©ì˜ ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    latest_data = df.sort_values('ë‚ ì§œ').groupby('ì¢…ëª©ëª…').last()
    industry_counts = latest_data.groupby('ì—…ì¢…').size().sort_values(ascending=False)
    
    fig = go.Figure(go.Bar(
        x=industry_counts.index,
        y=industry_counts.values,
        text=industry_counts.values,
        textposition='auto',
    ))
    
    fig.update_layout(
        title='ì—…ì¢…ë³„ ì¢…ëª© ìˆ˜ ë¶„í¬',
        xaxis_title='ì—…ì¢…',
        yaxis_title='ì¢…ëª© ìˆ˜',
        xaxis_tickangle=-45,
        height=600  # ì—…ì¢…ëª…ì´ ì˜ ë³´ì´ë„ë¡ ë†’ì´ ì¡°ì •
    )
    return fig

# --- Streamlit UI ---

# ì•± ì‹œì‘ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
init_database()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_date' not in st.session_state:
    st.session_state.analysis_date = None
if 'all_market_data' not in st.session_state:
    st.session_state.all_market_data = None

def read_google_sheet(worksheet_name=None):
    sheet = get_google_sheet()
    if not sheet:
        st.error("êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨")
        return None, []
    worksheet_list = sheet.worksheets()
    worksheet_names = [ws.title for ws in worksheet_list]
    if not worksheet_names:
        st.warning("êµ¬ê¸€ ì‹œíŠ¸ì— ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, worksheet_names
    # ì›Œí¬ì‹œíŠ¸ ì„ íƒ
    if worksheet_name is None:
        worksheet_name = worksheet_names[-1]  # ê¸°ë³¸ê°’: ë§ˆì§€ë§‰ ì›Œí¬ì‹œíŠ¸
    worksheet = sheet.worksheet(worksheet_name)
    data = worksheet.get_all_records()
    if not data:
        st.warning(f"{worksheet_name} ì›Œí¬ì‹œíŠ¸ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, worksheet_names
    import pandas as pd
    df = pd.DataFrame(data)
    return df, worksheet_names

# 4ê°œì˜ íƒ­ ìƒì„± (ë³µì›)
tab1, tab2, tab3, tab4 = st.tabs(["ì‹¤ì‹œê°„ ë¶„ì„", "ë°ì´í„°ë² ì´ìŠ¤", "ì¸í¬ê·¸ë˜í”½", "êµ¬ê¸€ ì‹œíŠ¸ ë³´ê¸°"])

# ì‹¤ì‹œê°„ ë¶„ì„ íƒ­
with tab1:
    # ë¶„ì„ ì„¤ì •
    col1, col2, col3 = st.columns(3)
    with col1:
        input_date = st.date_input(
            "ì¡°íšŒ ë‚ ì§œ",
            value=datetime.now().date(),
            format="YYYY-MM-DD"
        )
    with col2:
        top_n_count = st.number_input(
            "ìƒìœ„ ì¢…ëª©ìˆ˜",
            min_value=1,
            max_value=100,
            value=40,
            step=1
        )
    with col3:
        news_display_count = st.number_input(
            "íŠ¹ì§•ì£¼ ê¸°ì‚¬ ê²€ìƒ‰ìˆ˜",
            min_value=1,
            max_value=1000,
            value=500,
            step=1
        )

    # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ë‚˜ë€íˆ ë°°ì¹˜
    col1, _, _ = st.columns([2,1,1])
    with col1:
        run_analysis = st.button("ë¶„ì„ ì‹¤í–‰", type="primary")

    # ë¶„ì„ ì‹¤í–‰
    if run_analysis:
        try:
            # ì…ë ¥ê°’ ê²€ì¦
            date_str = input_date.strftime("%Y%m%d")
            if not is_valid_date_format(date_str):
                st.error("ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ì…ë‹ˆë‹¤.")
                st.stop()

            progress_text = "ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤..."
            progress_bar = st.progress(0, text=progress_text)

            # ì „ì²´ ì‹œì¥ ë°ì´í„° ì¡°íšŒ
            progress_bar.progress(0.5, text="ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì¤€ë¹„ ì¤‘...")
            all_market_data_df = get_all_market_data_with_names(date_str, company_details_df_global)
            if all_market_data_df is None or all_market_data_df.empty:
                st.error(f"{date_str} ë‚ ì§œì˜ ì‹œì¥ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            progress_bar.progress(1.0, text="ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ")

            # ë“±ë½ë¥  ê¸°ì¤€ ìƒìœ„ Nê°œ ì¢…ëª© ì„ íƒ
            progress_bar.progress(0.20, text="ìƒìœ„ ì¢…ëª© ì„ ë³„ ì¤‘...")
            top_n_df = all_market_data_df.sort_values(by='ë“±ë½ë¥ ', ascending=False).head(top_n_count)
            top_n_stock_names = set(top_n_df['ì¢…ëª©ëª…'].tolist())
            progress_bar.progress(0.30, text="ìƒìœ„ ì¢…ëª© ì„ ë³„ ì™„ë£Œ")

            # íŠ¹ì§•ì£¼ ë‰´ìŠ¤ ê²€ìƒ‰
            progress_bar.progress(0.35, text="íŠ¹ì§•ì£¼ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤€ë¹„ ì¤‘...")
            featured_stock_info = {}
            if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
                progress_bar.progress(0.40, text="ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ ì¤‘...")
                news_articles = call_naver_search_api("íŠ¹ì§•ì£¼", news_display_count, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)
                progress_bar.progress(0.50, text="íŠ¹ì§•ì£¼ ì •ë³´ ì¶”ì¶œ ì¤‘...")
                featured_stock_info = extract_featured_stock_names_from_news(news_articles, date_str, set(all_market_data_df['ì¢…ëª©ëª…']))
            progress_bar.progress(0.60, text="íŠ¹ì§•ì£¼ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ")

            # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ìƒì„±
            progress_bar.progress(0.85, text="ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘...")
            final_data_list = []
            processed_stocks = set()

            # Top N ì¢…ëª© ì²˜ë¦¬
            progress_bar.progress(0.35, text="Top N ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            for idx, (_, row) in enumerate(top_n_df.iterrows(), 1):
                stock_name = row['ì¢…ëª©ëª…']
                if stock_name in processed_stocks:
                    continue

                progress_bar.progress(0.35 + (idx/len(top_n_df))*0.20,
                    text=f"Top N ì¢…ëª© ì²˜ë¦¬ ì¤‘... ({idx}/{len(top_n_df)}) - {stock_name}")

                processed_stocks.add(stock_name)
                stock_info = {
                    'ë‚ ì§œ': date_str,
                    'í‹°ì»¤': row['í‹°ì»¤'],
                    'ì¢…ëª©ëª…': stock_name,
                    'ì—…ì¢…': row.get('ì—…ì¢…', ''),
                    'ì£¼ìš”ì œí’ˆ': row.get('ì£¼ìš”ì œí’ˆ', ''),
                    'ì‹œê°€': row['ì‹œê°€'],
                    'ê³ ê°€': row['ê³ ê°€'],
                    'ì €ê°€': row['ì €ê°€'],
                    'ì¢…ê°€': row['ì¢…ê°€'],
                    'ë“±ë½ë¥ ': row['ë“±ë½ë¥ '],
                    'ê±°ë˜ëŸ‰': row['ê±°ë˜ëŸ‰'],
                    'ê±°ë˜ëŒ€ê¸ˆ': row['ê±°ë˜ëŒ€ê¸ˆ'],
                    'ì‹œì¥': row['ì‹œì¥'],
                    'ë¹„ê³ ': ''
                }

                # ê¸°ì‚¬ ì»¬ëŸ¼ ì´ˆê¸°í™”
                initialize_article_columns(stock_info)

                if stock_name in featured_stock_info:
                    stock_info['ë¹„ê³ '] = f"top{top_n_count}+íŠ¹ì§•ì£¼"
                    # ì²« ë²ˆì§¸ ê¸°ì‚¬ëŠ” íŠ¹ì§•ì£¼ ê¸°ì‚¬ë¡œ ì„¤ì •
                    first_article = featured_stock_info[stock_name][0]
                    stock_info['ê¸°ì‚¬ì œëª©1'] = first_article['title']
                    stock_info['ê¸°ì‚¬ìš”ì•½1'] = first_article['description']
                    stock_info['ê¸°ì‚¬ë§í¬1'] = first_article['link']

                    # ì¶”ê°€ ê¸°ì‚¬ 4ê°œ ê²€ìƒ‰
                    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
                        progress_bar.progress(0.35 + (idx/len(top_n_df))*0.20,
                            text=f"Top N ì¢…ëª© ì¶”ê°€ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... ({idx}/{len(top_n_df)}) - {stock_name}")
                        additional_articles = search_stock_articles_by_date(
                            stock_name,
                            NAVER_CLIENT_ID,
                            NAVER_CLIENT_SECRET,
                            date_str,
                            max_count=4,
                            match_date=True
                        )
                        # ê¸°ì‚¬2~5ì— ë§¤í•‘
                        for i, article in enumerate(additional_articles, 2):
                            stock_info[f'ê¸°ì‚¬ì œëª©{i}'] = article['title']
                            stock_info[f'ê¸°ì‚¬ìš”ì•½{i}'] = article['description']
                            stock_info[f'ê¸°ì‚¬ë§í¬{i}'] = article['link']
                else:
                    stock_info['ë¹„ê³ '] = f"top{top_n_count}"
                    # ì¼ë°˜ ì¢…ëª©ì€ ê¸°ì‚¬ 5ê°œ ê²€ìƒ‰
                    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
                        progress_bar.progress(0.35 + (idx/len(top_n_df))*0.20,
                            text=f"Top N ì¢…ëª© ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... ({idx}/{len(top_n_df)}) - {stock_name}")
                        articles = search_stock_articles_by_date(
                            stock_name,
                            NAVER_CLIENT_ID,
                            NAVER_CLIENT_SECRET,
                            date_str,
                            max_count=5,
                            match_date=True
                        )
                        for i, article in enumerate(articles, 1):
                            stock_info[f'ê¸°ì‚¬ì œëª©{i}'] = article['title']
                            stock_info[f'ê¸°ì‚¬ìš”ì•½{i}'] = article['description']
                            stock_info[f'ê¸°ì‚¬ë§í¬{i}'] = article['link']

                final_data_list.append(stock_info)
            progress_bar.progress(0.55, text="Top N ì¢…ëª© ì²˜ë¦¬ ì™„ë£Œ")

            # íŠ¹ì§•ì£¼ ì •ë³´ ì¶”ê°€
            progress_bar.progress(0.60, text="íŠ¹ì§•ì£¼ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
            featured_stocks = [stock for stock in featured_stock_info.keys() if stock not in processed_stocks]
            for idx, stock_name in enumerate(featured_stocks, 1):
                progress_bar.progress(0.60 + (idx/len(featured_stocks))*0.20,
                    text=f"íŠ¹ì§•ì£¼ ì •ë³´ ì²˜ë¦¬ ì¤‘... ({idx}/{len(featured_stocks)}) - {stock_name}")
                try:
                    stock_row = all_market_data_df[all_market_data_df['ì¢…ëª©ëª…'] == stock_name].iloc[0]
                    stock_info = {
                        'ë‚ ì§œ': date_str,
                        'í‹°ì»¤': stock_row['í‹°ì»¤'],
                        'ì¢…ëª©ëª…': stock_name,
                        'ì—…ì¢…': stock_row.get('ì—…ì¢…', ''),
                        'ì£¼ìš”ì œí’ˆ': stock_row.get('ì£¼ìš”ì œí’ˆ', ''),
                        'ì‹œê°€': stock_row['ì‹œê°€'],
                        'ê³ ê°€': stock_row['ê³ ê°€'],
                        'ì €ê°€': stock_row['ì €ê°€'],
                        'ì¢…ê°€': stock_row['ì¢…ê°€'],
                        'ë“±ë½ë¥ ': stock_row['ë“±ë½ë¥ '],
                        'ê±°ë˜ëŸ‰': stock_row['ê±°ë˜ëŸ‰'],
                        'ê±°ë˜ëŒ€ê¸ˆ': stock_row['ê±°ë˜ëŒ€ê¸ˆ'],
                        'ì‹œì¥': stock_row['ì‹œì¥'],
                        'ë¹„ê³ ': 'íŠ¹ì§•ì£¼'
                    }

                    # ê¸°ì‚¬ ì»¬ëŸ¼ ì´ˆê¸°í™”
                    initialize_article_columns(stock_info)

                    # ì²« ë²ˆì§¸ ê¸°ì‚¬ëŠ” íŠ¹ì§•ì£¼ ê¸°ì‚¬
                    first_article = featured_stock_info[stock_name][0]
                    stock_info['ê¸°ì‚¬ì œëª©1'] = first_article['title']
                    stock_info['ê¸°ì‚¬ìš”ì•½1'] = first_article['description']
                    stock_info['ê¸°ì‚¬ë§í¬1'] = first_article['link']

                    # ì¶”ê°€ ê¸°ì‚¬ 4ê°œ ê²€ìƒ‰
                    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
                        progress_bar.progress(0.60 + (idx/len(featured_stocks))*0.20,
                            text=f"íŠ¹ì§•ì£¼ ì¶”ê°€ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... ({idx}/{len(featured_stocks)}) - {stock_name}")
                        additional_articles = search_stock_articles_by_date(
                            stock_name,
                            NAVER_CLIENT_ID,
                            NAVER_CLIENT_SECRET,
                            date_str,
                            max_count=4,
                            match_date=True
                        )

                        # ê¸°ì‚¬2~5ì— ë§¤í•‘
                        for i, article in enumerate(additional_articles, 2):
                            stock_info[f'ê¸°ì‚¬ì œëª©{i}'] = article['title']
                            stock_info[f'ê¸°ì‚¬ìš”ì•½{i}'] = article['description']
                            stock_info[f'ê¸°ì‚¬ë§í¬{i}'] = article['link']

                    final_data_list.append(stock_info)
                    processed_stocks.add(stock_name)
                except Exception as e:
                    st.error(f"íŠ¹ì§•ì£¼ {stock_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    continue
            progress_bar.progress(0.80, text="íŠ¹ì§•ì£¼ ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ")

            # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬
            progress_bar.progress(0.85, text="ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘...")
            final_df = pd.DataFrame(final_data_list)
            progress_bar.progress(0.90, text="ë°ì´í„° ì •ë ¬ ì¤‘...")
            final_df_sorted = final_df.sort_values(by='ë“±ë½ë¥ ', ascending=False)

            progress_bar.progress(0.95, text="ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")

            # ë¶„ì„ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.analysis_results = final_df_sorted
            st.session_state.analysis_date = date_str
            st.session_state.all_market_data = all_market_data_df

            progress_bar.progress(1.0, text="ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            time.sleep(1)
            progress_bar.empty()

            # ê²°ê³¼ í‘œì‹œ
            display_analysis_results(final_df_sorted, date_str, all_market_data_df, top_n_count)

        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.error("ìƒì„¸ ì˜¤ë¥˜:")
            st.exception(e)

    # ì´ì „ ë¶„ì„ ê²°ê³¼ í‘œì‹œ (ì„¸ì…˜ì— ì €ì¥ëœ ê²°ê³¼ê°€ ìˆì„ ê²½ìš°)
    elif st.session_state.analysis_results is not None:
        display_analysis_results(
            st.session_state.analysis_results,
            st.session_state.analysis_date,
            st.session_state.all_market_data,
            top_n_count
        )

# ë°ì´í„°ë² ì´ìŠ¤ íƒ­
with tab2:
    saved_dates = get_saved_dates()
    st.subheader("ê¸‰ë“±ì£¼+íŠ¹ì§•ì£¼ ë¶„ì„ ê²°ê³¼ ê¸°ê°„ë³„ ì¡°íšŒ")
    # ì €ì¥ëœ ë‚ ì§œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if saved_dates:
        # ê¸°ê°„ ì„ íƒ UI
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input(
                "ì‹œì‘ ë‚ ì§œ",
                value=datetime.strptime(min(saved_dates), '%Y%m%d').date(),
                min_value=datetime.strptime(min(saved_dates), '%Y%m%d').date(),
                max_value=datetime.strptime(max(saved_dates), '%Y%m%d').date(),
                format="YYYY-MM-DD"
            )
        with col2:
            end_date = st.date_input(
                "ì¢…ë£Œ ë‚ ì§œ",
                value=datetime.strptime(max(saved_dates), '%Y%m%d').date(),
                min_value=datetime.strptime(min(saved_dates), '%Y%m%d').date(),
                max_value=datetime.strptime(max(saved_dates), '%Y%m%d').date(),
                format="YYYY-MM-DD"
            )

        if start_date <= end_date:
            start_date_str = start_date.strftime('%Y%m%d')
            end_date_str = end_date.strftime('%Y%m%d')
            period_data = get_data_by_date_range(start_date_str, end_date_str)

            if not period_data.empty:
                # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬: ì¢…ëª©ëª… ë’¤ì— í…Œë§ˆ, AI_í•œì¤„ìš”ì•½
                db_columns = [
                    'ë‚ ì§œ', 'í‹°ì»¤', 'ì¢…ëª©ëª…',
                    'ì—…ì¢…', 'ì£¼ìš”ì œí’ˆ', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ', 'ì‹œì¥', 'ë¹„ê³ ',
                    'ê¸°ì‚¬ì œëª©1', 'ê¸°ì‚¬ìš”ì•½1', 'ê¸°ì‚¬ë§í¬1',
                    'ê¸°ì‚¬ì œëª©2', 'ê¸°ì‚¬ìš”ì•½2', 'ê¸°ì‚¬ë§í¬2',
                    'ê¸°ì‚¬ì œëª©3', 'ê¸°ì‚¬ìš”ì•½3', 'ê¸°ì‚¬ë§í¬3',
                    'ê¸°ì‚¬ì œëª©4', 'ê¸°ì‚¬ìš”ì•½4', 'ê¸°ì‚¬ë§í¬4',
                    'ê¸°ì‚¬ì œëª©5', 'ê¸°ì‚¬ìš”ì•½5', 'ê¸°ì‚¬ë§í¬5'
                ]
                for col in db_columns:
                    if col not in period_data.columns:
                        period_data[col] = ""
                period_data = period_data[db_columns]

                # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
                display_df = period_data.copy()
                numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']
                for col in numeric_columns:
                    if col in display_df.columns:
                        display_df[col] = display_df[col].apply(format_number)
                display_df['ë“±ë½ë¥ '] = display_df['ë“±ë½ë¥ '].apply(format_percentage)

                styled_df = display_df.style.map(color_negative_red, subset=['ë“±ë½ë¥ '])
                st.dataframe(styled_df, use_container_width=True)

                # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
                col1, col2 = st.columns(2)
                with col1:
                    excel_data = get_excel_data(period_data, f"{start_date_str}-{end_date_str}")
                    st.download_button(
                        label="Excel ë‹¤ìš´ë¡œë“œ",
                        data=excel_data,
                        file_name=f"stock_analysis_{start_date_str}-{end_date_str}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                with col2:
                    txt_data = get_txt_data(period_data)
                    st.download_button(
                        label="TXT ë‹¤ìš´ë¡œë“œ",
                        data=txt_data,
                        file_name=f"stock_analysis_{start_date_str}-{end_date_str}.txt",
                        mime="text/plain"
                    )
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error("ì¢…ë£Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
    else:
        st.info("ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì¸í¬ê·¸ë˜í”½ íƒ­
with tab3:
    saved_dates = get_saved_dates()
    st.subheader("ê¸‰ë“±ì£¼+íŠ¹ì§•ì£¼ ê¸°ê°„ë³„ ë¶„ì„ ì¸í¬ê·¸ë˜í”½")
    # ì €ì¥ëœ ì „ì²´ ë‚ ì§œ ë²”ìœ„ í™•ì¸
    if saved_dates:
        saved_dates_dt = [datetime.strptime(date, '%Y%m%d').date() for date in saved_dates]
        min_date = min(saved_dates_dt)
        max_date = max(saved_dates_dt)

        # ê¸°ê°„ ì„ íƒ UI
        col1, col2 = st.columns(2)
        with col1:
            # ì‹œì‘ ë‚ ì§œëŠ” max_dateë³´ë‹¤ í•˜ë£¨ ì „ìœ¼ë¡œ ì„¤ì •
            default_start_date = max_date if max_date == min_date else max_date - timedelta(days=1)
            viz_start_date = st.date_input(
                "ì‹œì‘ ë‚ ì§œ",
                value=default_start_date,
                min_value=min_date,
                max_value=max_date,
                format="YYYY-MM-DD",
                key="viz_start_date"
            )
        with col2:
            viz_end_date = st.date_input(
                "ì¢…ë£Œ ë‚ ì§œ",
                value=max_date,
                min_value=min_date,
                max_value=max_date,
                format="YYYY-MM-DD",
                key="viz_end_date"
            )

        if viz_start_date <= viz_end_date:
            # ì„ íƒëœ ê¸°ê°„ì˜ ë°ì´í„° ì¡°íšŒ
            viz_start_date_str = viz_start_date.strftime('%Y%m%d')
            viz_end_date_str = viz_end_date.strftime('%Y%m%d')
            period_data = get_data_by_date_range(viz_start_date_str, viz_end_date_str)

            if not period_data.empty:
                # 4ê°œì˜ ì°¨íŠ¸ë¥¼ 2x2 ê·¸ë¦¬ë“œë¡œ ë°°ì¹˜
                col1, col2 = st.columns(2)
                with col1:
                    st.plotly_chart(create_market_distribution_pie(period_data), use_container_width=True)
                    st.plotly_chart(create_top_volume_bar(period_data), use_container_width=True)
                with col2:
                    st.plotly_chart(create_top_rate_changes_bar(period_data), use_container_width=True)
                    st.plotly_chart(create_industry_distribution_bar(period_data), use_container_width=True)
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error("ì¢…ë£Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
    else:
        st.info("ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# êµ¬ê¸€ ì‹œíŠ¸ ë³´ê¸° íƒ­
with tab4:
    st.subheader("êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° ë³´ê¸°")
    # ì›Œí¬ì‹œíŠ¸ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì„ íƒ
    sheet = get_google_sheet()
    if sheet:
        worksheet_list = sheet.worksheets()
        worksheet_names = [ws.title for ws in worksheet_list]
        if worksheet_names:
            selected_ws = st.selectbox("ì›Œí¬ì‹œíŠ¸ ì„ íƒ", worksheet_names, index=len(worksheet_names)-1)
            df, _ = read_google_sheet(selected_ws)
            if df is not None and not df.empty:
                st.dataframe(df, use_container_width=True)
                st.success(f"{selected_ws} ì›Œí¬ì‹œíŠ¸ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            else:
                st.warning(f"{selected_ws} ì›Œí¬ì‹œíŠ¸ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("êµ¬ê¸€ ì‹œíŠ¸ì— ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.error("êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨")

# ë„ì›€ë§
with st.expander("ë„ì›€ë§"):
    st.markdown("""
    ## ğŸ“– ì‚¬ìš© ê°€ì´ë“œ

    ì´ ì•±ì€ ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„°ì™€ ë‰´ìŠ¤ ë¶„ì„ì„ í†µí•´ ê¸‰ë“±ì£¼ì™€ íŠ¹ì§•ì£¼ë¥¼ íƒì§€í•˜ê³ , ë‹¤ì–‘í•œ ì‹œê°í™”ì™€ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

    ---
    ### 1ï¸âƒ£ ì‹¤ì‹œê°„ ë¶„ì„ íƒ­
    - **ì¡°íšŒ ë‚ ì§œ**: ë¶„ì„í•  ë‚ ì§œë¥¼ ì„ íƒí•©ë‹ˆë‹¤. (ì˜ì—…ì¼ ê¸°ì¤€, ì¥ì¤‘ì—ëŠ” ê¸°ì‚¬ ìˆ˜ê°€ ì ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
    - **ìƒìœ„ ì¢…ëª©ìˆ˜**: ë“±ë½ë¥  ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ëª‡ ê°œ ì¢…ëª©ì„ ë¶„ì„í• ì§€ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: 40)
    - **íŠ¹ì§•ì£¼ ê¸°ì‚¬ ê²€ìƒ‰ìˆ˜**: ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ 'íŠ¹ì§•ì£¼' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•  ê¸°ì‚¬ ìˆ˜ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (ì˜ˆ: 500)
    - **ë¶„ì„ ì‹¤í–‰**: ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„°ì™€ ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.
    - **ë¶„ì„ ê²°ê³¼**: 
        - 'ê¸‰ë“±ì£¼+íŠ¹ì§•ì£¼ ë¶„ì„' íƒ­ì—ì„œ Top N ì¢…ëª©ê³¼ íŠ¹ì§•ì£¼, ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬, ë°ì´í„° ë‚´ë³´ë‚´ê¸°(Excel, TXT, DB ì €ì¥) ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
        - 'ì „ì²´ ì¢…ëª© ë¶„ì„' íƒ­ì—ì„œ ì „ì²´ ì‹œì¥ ë°ì´í„°, Top30 ë“±ë½ë¥ /ê±°ë˜ëŒ€ê¸ˆ, íˆ¬ììë³„ ê±°ë˜ëŒ€ê¸ˆ, ì‹œì¥ë³„ íˆ¬ìì ì •ë³´ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ---
    ### 2ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ íƒ­
    - **ê¸°ê°„ë³„ ì¡°íšŒ**: ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œì‘/ì¢…ë£Œ ë‚ ì§œë¡œ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **ê²°ê³¼ í…Œì´ë¸”**: í•´ë‹¹ ê¸°ê°„ì˜ ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í‘œë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **ë°ì´í„° ë‚´ë³´ë‚´ê¸°**: Excel, TXT íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ---
    ### 3ï¸âƒ£ ì¸í¬ê·¸ë˜í”½ íƒ­
    - **ê¸°ê°„ ì„ íƒ**: ì €ì¥ëœ ë°ì´í„° ì¤‘ ì›í•˜ëŠ” ê¸°ê°„ì„ ì„ íƒí•©ë‹ˆë‹¤.
    - **ì‹œê°í™”**: ì‹œì¥ë³„ ì¢…ëª© ë¶„í¬, ë“±ë½ë¥  ìƒìœ„ 10ê°œ, ê±°ë˜ëŸ‰ ìƒìœ„ 10ê°œ, ì—…ì¢…ë³„ ì¢…ëª© ìˆ˜ ë¶„í¬ ë“± ë‹¤ì–‘í•œ ì°¨íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    ---
    ### ì£¼ìš” ê¸°ëŠ¥ ì„¤ëª…
    - **Top N + íŠ¹ì§•ì£¼**: ë“±ë½ë¥  ìƒìœ„ Nê°œ ì¢…ëª©ê³¼ ë‰´ìŠ¤ì—ì„œ íŠ¹ì§•ì£¼ë¡œ ì–¸ê¸‰ëœ ì¢…ëª©
    - **íŠ¹ì§•ì£¼**: ë„¤ì´ë²„ ê¸°ì‚¬ì—ì„œ íŠ¹ì§•ì£¼ë¡œ ì–¸ê¸‰ëœ ì¢…ëª©
    - **DB ì €ì¥**: ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ì¡°íšŒ/ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **Excel/TXT ë‹¤ìš´ë¡œë“œ**: ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ---
    ### ê²°ê³¼ í•´ì„ íŒ
    - **ê±°ë˜ëŒ€ê¸ˆ 100ì–µ ì´ìƒ**: ëŒ€í˜• ê±°ë˜ê°€ ë°œìƒí•œ ì¢…ëª©ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **Top N + íŠ¹ì§•ì£¼/íŠ¹ì§•ì£¼**: ë‰´ìŠ¤ì™€ ì‹œì¥ ë°ì´í„°ê°€ ë™ì‹œì— ì£¼ëª©í•˜ëŠ” ì¢…ëª©ì„ í™•ì¸í•˜ì„¸ìš”.
    - **ì‹œì¥ë³„/ì—…ì¢…ë³„ ë¶„í¬**: íŠ¹ì • ì‹œì¥ì´ë‚˜ ì—…ì¢…ì— ê¸‰ë“±ì£¼ê°€ ëª°ë ¤ ìˆëŠ”ì§€ í•œëˆˆì— ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ---
    ### ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)
    - **Q. ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆì–´ìš”!**
        - ì˜ì—…ì¼ì´ ì•„ë‹ˆê±°ë‚˜, ì¥ ë§ˆê° ì „ì—ëŠ” ë°ì´í„°/ë‰´ìŠ¤ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë˜ëŠ” API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.
    - **Q. DB ì €ì¥ì´ ì•ˆ ë¼ìš”!**
        - ê°™ì€ ë‚ ì§œ ë°ì´í„°ê°€ ì´ë¯¸ ì €ì¥ëœ ê²½ìš°, ë®ì–´ì“°ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.
    - **Q. ë‰´ìŠ¤ê°€ ë„ˆë¬´ ì ê²Œ ë‚˜ì™€ìš”!**
        - ê¸°ì‚¬ ê²€ìƒ‰ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, ì¥ ë§ˆê° í›„ì— ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.

    ---
    ### ë¬¸ì˜ ë° í”¼ë“œë°±
    - ì˜¤ë¥˜/ê±´ì˜ì‚¬í•­ì€ ê°œë°œìì—ê²Œ ì§ì ‘ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.
    - [ì´ë©”ì¼: hellolk2000@gmail.com]
    """)

def read_google_sheet(worksheet_name=None):
    sheet = get_google_sheet()
    if not sheet:
        st.error("êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨")
        return None
    # ì›Œí¬ì‹œíŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    worksheet_list = sheet.worksheets()
    worksheet_names = [ws.title for ws in worksheet_list]
    if not worksheet_names:
        st.warning("êµ¬ê¸€ ì‹œíŠ¸ì— ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    # ì›Œí¬ì‹œíŠ¸ ì„ íƒ
    if worksheet_name is None:
        worksheet_name = worksheet_names[-1]  # ê¸°ë³¸ê°’: ë§ˆì§€ë§‰ ì›Œí¬ì‹œíŠ¸
    worksheet = sheet.worksheet(worksheet_name)
    data = worksheet.get_all_records()
    if not data:
        st.warning(f"{worksheet_name} ì›Œí¬ì‹œíŠ¸ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    import pandas as pd
    df = pd.DataFrame(data)
    return df, worksheet_names


